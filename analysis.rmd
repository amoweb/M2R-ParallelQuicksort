# Performance analysis of the Parallel quick sort algorithm

```{r echo=FALSE, message=FALSE}
library(ggplot2)
library(plyr)
```

## Basis analysis
This software is composed of three algorithms:

* A sequential sort algorithm
* The libc sequential sort algorithm
* The parallel version


```{r echo=FALSE, message=FALSE}
dfw <- read.csv("data/amoweb_2014-11-14/measurements_20:58_wide.txt",header=T)
dfw4 <- dfw[dfw$Threads<4,]
dfwHigh <- dfw[dfw$Size>1000000,]
dfwHigh4 <- dfwHigh[dfwHigh$Threads<4,]
dfwBig <- read.csv("data/amoweb_2014-11-16/measurements_20:19_wide.txt",header=T)
dfwBig4 <- dfwBig[dfwBig$Threads<4,]
```

We execute several time the program computing with each of the tree versions. Each execution have several dimensions:

* The number of Thread (from 0 for no parallelism, to 3 for quad-thread execution).
* The size of the array
* The execution time (for sequential, LibC sequential and parallel versions)

The machine is a quad-core hyperthread.

```{r, echo=FALSE}
#names(dfw)
#plot(dfw[names(dfw) %in% c("Par", "Threads","Size")])

```

The first figure shows the execution time for several threads. It shows for a little array, multi-thread is counter-productive. We use a logarithmic scale to show these little values.


```{r}
ggplot(data = dfw4, aes(x=factor(Threads), y=Par)) + scale_y_continuous(trans="log2") +
geom_boxplot() + facet_wrap(~Size);
```

The same figure, with a linear scale shows the threads are useful for big array. 

```{r}
ggplot(data = dfwHigh4, aes(x=factor(Threads), y=Par)) +
geom_boxplot() + facet_wrap(~Size);
```

Now, we display, for a size of 10e8, the duration for each number of thread. We see the average computation duration is decreasing.

```{r}
ggplot(data = dfwBig4, aes(x=factor(Threads), y=Par)) + geom_boxplot();
```